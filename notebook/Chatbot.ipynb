{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1be2eec8-8040-4bfe-a131-b8fd5c95d55c",
   "metadata": {},
   "source": [
    "Building a simple RAG chatbot with LangChain, Hugging Face, FAISS, Amazon SageMaker and Amazon Textract"
   ]
  },
  {
   "cell_type": "raw",
   "id": "810b94f7-7906-4a78-b845-c64deb82a51a",
   "metadata": {},
   "source": [
    "Deploy Mistral 7b instruct using jumpstart model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c6ebc62-ab8b-48bd-a548-9acebed9536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install sagemaker langchain langchain-community amazon-textract-caller amazon-textract-textractor sentence-transformers pypdf faiss-cpu==1.8.0 -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34140ac1-a626-49c6-8c0b-683a44b165ab",
   "metadata": {},
   "source": [
    "Imports required for RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b58f3864-e5d4-4d94-92c9-fb99fe3d85e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3, json, sagemaker\n",
    "import os\n",
    "\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from transformers import AutoConfig\n",
    "from typing import Dict\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.llms import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_community.document_loaders import AmazonTextractPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88a0d46e-6b82-408e-8c9d-d61153c00aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "#SageMaker JumpStart provides APIs as part of SageMaker SDK that allow you to deploy and fine-tune models in network isolation using scripts that SageMaker maintains.\n",
    "\n",
    "model = JumpStartModel(model_id=\"huggingface-llm-mistral-7b-instruct-v3\", instance_type='ml.g5.2xlarge',model_version='1.1.1')\n",
    "example_payloads = model.retrieve_all_examples()\n",
    "\n",
    "predictor = model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1dc87f-3415-46e7-a910-44c8c45a128c",
   "metadata": {},
   "source": [
    " Configure the LangChain input and output handlers for our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c37a97f-9fad-4a9e-af0f-4c55dfd0a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\"max_new_tokens\": 512, \"top_p\": 0.8, \"temperature\": 0.8}\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps(\n",
    "            # Mistral prompt, see https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\n",
    "            {\"inputs\": f\"<s>[INST] {prompt} [/INST]\", \"parameters\": {**model_kwargs}}\n",
    "        )\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        splits = response_json[0][\"generated_text\"].split(\"[/INST] \")\n",
    "        return splits[1]\n",
    "\n",
    "content_handler = ContentHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ab52325-98a9-4081-9465-8396fd31d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client('sagemaker')\n",
    "smrt_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "llm = SagemakerEndpoint(\n",
    "    endpoint_name=predictor.endpoint_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    content_handler=content_handler,\n",
    "    client=smrt_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "005bbf04-8246-476e-8a87-baf0ea448f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSagemakerEndpoint\u001b[0m\n",
      "Params: {'endpoint_name': 'hf-llm-mistral-7b-instruct-v3-2024-11-03-11-11-44-874', 'model_kwargs': {'max_new_tokens': 512, 'top_p': 0.8, 'temperature': 0.8}} hf-llm-mistral-7b-instruct-v3-2024-11-03-11-11-44-874\n"
     ]
    }
   ],
   "source": [
    "print(llm, llm.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1442713b-a181-418e-bb8d-621504df80f1",
   "metadata": {},
   "source": [
    "RAG example with PDF files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333517e-4d94-4d00-be39-ddc9d710191e",
   "metadata": {},
   "source": [
    "Upload local PDF files to S3\n",
    "Sources:\n",
    "https://www.iea.org/reports/world-energy-investment-2024\n",
    "Feel free to use your own files, the code below should work without any change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d53806b0-3bf0-4b1a-b951-dacc1ba18307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 bucket and prefix for PDF storage\n",
    "\n",
    "bucket = \"aits-mr-tankwar-chatbot-730335476518-us-east-1\"\n",
    "prefix = \"RAG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ce35d94-33e3-4fdf-95f6-30a0aa584daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-03 02:41:29          0 \n",
      "2024-11-03 11:21:50     178490 Coverage_Medical.pdf\n",
      "upload: pdfs/.ipynb_checkpoints/Coverage_Medical-checkpoint.pdf to s3://aits-mr-tankwar-chatbot-730335476518-us-east-1/RAG/.ipynb_checkpoints/Coverage_Medical-checkpoint.pdf\n",
      "upload: pdfs/Coverage_Medical.pdf to s3://aits-mr-tankwar-chatbot-730335476518-us-east-1/RAG/Coverage_Medical.pdf\n"
     ]
    }
   ],
   "source": [
    "%%sh -s $bucket $prefix\n",
    "\n",
    "aws s3 ls s3://$1/$2/\n",
    "aws s3 cp --recursive pdfs s3://$1/$2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94bcb12d-f188-4a65-991d-0c8b37ee4ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://aits-mr-tankwar-chatbot-730335476518-us-east-1/RAG/.ipynb_checkpoints/Coverage_Medical-checkpoint.pdf',\n",
       " 's3://aits-mr-tankwar-chatbot-730335476518-us-east-1/RAG/Coverage_Medical.pdf']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build list of S3 URIs\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "objs = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "# Extract 'Contents' if any objects are found\n",
    "if 'Contents' in objs:\n",
    "    objs = objs['Contents']\n",
    "    uris = [f's3://{bucket}/{obj[\"Key\"]}' for obj in objs if not obj[\"Key\"].endswith('/')]\n",
    "else:\n",
    "    uris = []\n",
    "\n",
    "uris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c153e42-071d-4fc9-9f75-e30ed7d3d3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 pages, 67 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=0)\n",
    "all_chunks = []\n",
    "\n",
    "loader = PyPDFLoader(\"./pdfs/Coverage_Medical.pdf\")\n",
    "documents = loader.load()\n",
    "chunks = splitter.split_documents(documents)\n",
    "all_chunks += chunks\n",
    "print(f\"Loaded {len(documents)} pages, {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "315fa2e5-55fa-451d-a2c1-4f6546b35cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://aits-mr-tankwar-chatbot-730335476518-us-east-1/RAG/.ipynb_checkpoints/Coverage_Medical-checkpoint.pdf started\n",
      "An error occurred: Read timeout on endpoint URL: \"https://textract.us-east-1.amazonaws.com/\"\n",
      "s3://aits-mr-tankwar-chatbot-730335476518-us-east-1/RAG/Coverage_Medical.pdf started\n",
      "An error occurred: Read timeout on endpoint URL: \"https://textract.us-east-1.amazonaws.com/\"\n",
      "CPU times: user 149 ms, sys: 23.9 ms, total: 173 ms\n",
      "Wall time: 10min 9s\n"
     ]
    }
   ],
   "source": [
    "#not working\n",
    "%%time\n",
    "textract_client = boto3.client('textract', region_name=\"us-east-1\")\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=0)\n",
    "\n",
    "all_chunks = []\n",
    "for uri in uris:\n",
    "    try:\n",
    "        loader = AmazonTextractPDFLoader(uri, client=textract_client)\n",
    "        print(f\"{uri} started\")\n",
    "        documents = loader.load()\n",
    "        print(len(documents))\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb2268-cf2b-4f99-bf4c-9f3d3be0f1de",
   "metadata": {},
   "source": [
    "Embed document chunks and store them in FAISS\n",
    "https://github.com/facebookresearch/faiss"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f3e3f2a-7903-4e0a-a1cf-596ac027f9c8",
   "metadata": {},
   "source": [
    "# Define embedding model\n",
    "# See https://huggingface.co/spaces/mteb/leaderboard\n",
    "embedding_model_id = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ed289-d831-4a1e-9295-c2042bf3ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Embed chunks\n",
    "embeddings_db = FAISS.from_documents(all_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39f8ef-9ecc-4472-8c65-5d583aa29944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save database\n",
    "embeddings_db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f82825-c5de-4a21-b7cc-dcaf49c41c86",
   "metadata": {},
   "source": [
    "Shortcut : load existing embedding database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97718f24-ee48-4aef-a6a5-91f151386380",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_db = FAISS.load_local(\"faiss_index\", embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
